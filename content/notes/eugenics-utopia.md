---
title: "Eugenics and the Promise of Utopia through AGI"
date: 2023-04-12
tags:
- agi
- eugenics
- tescreal
---
[Secure and Trustworthy Machine Learning Keynote](https://www.youtube.com/watch?v=P7XT4TWLzJw&ab_channel=NicolasPapernot)
- Various AGI definitions: sounds like an unscoped system w/ the apparent goal of trying to do everything for everyone under any environment
- why would you want to build something so poorly defined?
	- short answer: rooted in 20th century eugenics
- first-wave eugenics
	- [Stanford Eugenics History Project](https://www.stanfordeugenics.com/)
	- [Francis Galton](https://en.wikipedia.org/wiki/Francis_Galton)
- TESCREAL: Transhumanism, Extropianism, Singularitanism, Cosmism, Rationalism, Effective Altruism, Longtermism
	- LessWrong: founded by [Yudkovsky](https://en.wikipedia.org/wiki/Eliezer_Yudkowsky)
		- goals: 'improving human reasoning & rationality training'
	- Effective Altruism / Longtermism: utility of far future lives in long-term drastically outweigh near 
- connections to eugenics
	- Historical roots of contemporary communities
		- common lineage to first-wave eugenics
		- overlap in communities
		- desire to radically modify human organism
	- Eschatology
		- Bostrom's superintelligence utopia & apocalypse
	- Discriminatory views
		- Bostrom's history of dysgenic views
		- Yudkowsky's statements on IQ
		- [PELTIV scores](https://www.vox.com/future-perfect/23569519/effective-altrusim-sam-bankman-fried-will-macaskill-ea-risk-decentralization-philanthropy)
	- AGI history
	![[notes/images/agi-history.png]]
		- 2015: Musk & Altman respond to DeepMind's acquisition by google, OpenAI founded as nonprofit
		- 2019: restructure as for profit; 1bn from microsoft
		- 2023: 10bn from microsoft
	- AGI Utopia... for who?
		- AGI race has started a race to the bottom to go-to-market faster and create larger models advertised as being able to do everything for everyone
		- overview of harms of LLMs & deepfakes
		- worker exploitation
		- centralization of power: resources do not serve organizations and communities, allocated to one company
	- AGI Apocalypse
		- existential risk
		- Concern over ethics routes those concerned about "ai safety" into TESCREAL organizations and career paths
		- pump resources into building AGI, harms marginalized groups in the process
		- framing as AGI allows companies to avoid accountability / responsibility: the artifact (the model) is anthropomorphised
	- Trying to "build" AGI is an inherently unsafe practice. Build well-scoped, well-defined systems instead. Don't attempt to build a god.