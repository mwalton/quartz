<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>values on</title><link>https://mwalton.me/tags/values/</link><description>Recent content in values on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 24 Mar 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://mwalton.me/tags/values/index.xml" rel="self" type="application/rss+xml"/><item><title>The Values Encoded in Machine Learning Research</title><link>https://mwalton.me/notes/ml-value-encoding/</link><pubDate>Fri, 24 Mar 2023 00:00:00 +0000</pubDate><guid>https://mwalton.me/notes/ml-value-encoding/</guid><description> paper
![[notes/images/paper-values.png]] ![[notes/images/social-need-harm.png]]</description></item><item><title>Rebuilding Society on Meaning</title><link>https://mwalton.me/notes/meaning/</link><pubDate>Thu, 16 Mar 2023 00:00:00 +0000</pubDate><guid>https://mwalton.me/notes/meaning/</guid><description>Rebuilding Society on Meaning
Ideas an interesting modeling approach could be combining an LM that converses with users encouraging them to introspect on their values combined w/ a theory of mind that the model constructs (and persists) of them between use sessions (this could also get quite dark) i like the idea of trying to align recommenders w/ what users actually value, however I&amp;rsquo;m concerned about how the existing incentive structure (advertising / attention economy / surveillance capitalism) will incorporate research in this area.</description></item></channel></rss>