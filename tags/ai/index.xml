<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ai on</title><link>https://mwalton.me/tags/ai/</link><description>Recent content in ai on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 19 Mar 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://mwalton.me/tags/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>Stochastic Parrots Day ðŸ¦œ (DAIR)</title><link>https://mwalton.me/notes/dair-stochastic-parrots/</link><pubDate>Sun, 19 Mar 2023 00:00:00 +0000</pubDate><guid>https://mwalton.me/notes/dair-stochastic-parrots/</guid><description>![[notes/images/stochastic-parrots.png]]
Retrospective Problem: presented as knowledge access systems; similar to issue w/ meta galactica for science Language: pairs of form and meaning, language models only have access to form Good at generating plausible sounding text; refined w/ preferences at producing non-offensive plausible sounding text Philosophers asking: â€œare the models thinking?</description></item><item><title>People + AI Research Symposium</title><link>https://mwalton.me/notes/PAIR/</link><pubDate>Sat, 18 Mar 2023 00:00:00 +0000</pubDate><guid>https://mwalton.me/notes/PAIR/</guid><description>Intentional ignorance is a value-laden choice, Margaret Mitchell
choice of dataset and curation define what is important model selection encodes additional bias generation of new data from users interacting with the model generates a bias feedback loop over and underrepresentation of subpopulations encode value judgements foresight is critical: intended use / users foreseeable use / users foreseeable discriminatory uses risks foreseeable harms foreseeable disproportionate harms &amp;ldquo;I&amp;rsquo;m just an engineer / data scientist&amp;rdquo; is willful ignorance; inflicts foreseeable harm !</description></item><item><title>Rebuilding Society on Meaning</title><link>https://mwalton.me/notes/meaning/</link><pubDate>Thu, 16 Mar 2023 00:00:00 +0000</pubDate><guid>https://mwalton.me/notes/meaning/</guid><description>Rebuilding Society on Meaning
Ideas an interesting modeling approach could be combining an LM that converses with users encouraging them to introspect on their values combined w/ a theory of mind that the model constructs (and persists) of them between use sessions (this could also get quite dark) i like the idea of trying to align recommenders w/ what users actually value, however I&amp;rsquo;m concerned about how the existing incentive structure (advertising / attention economy / surveillance capitalism) will incorporate research in this area.</description></item><item><title>Alignment</title><link>https://mwalton.me/thoughts/alignment/</link><pubDate>Mon, 06 Mar 2023 00:00:00 +0000</pubDate><guid>https://mwalton.me/thoughts/alignment/</guid><description>ðŸš§ TODO ðŸš§
Central themes [[thoughts/participatory-design]] [[thoughts/transparency]] [[thoughts/biomimicry]] Contestable AI contestable.ai Meaning Alignment there are some interesting emerging ideas around [[notes/meaning|meaning]] aligned machine learning; I&amp;rsquo;m cautiously optimistic on this direction depending on what organizations get involved coopts the concept</description></item></channel></rss>