<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>alignment on</title><link>https://mwalton.me/tags/alignment/</link><description>Recent content in alignment on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 16 Mar 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://mwalton.me/tags/alignment/index.xml" rel="self" type="application/rss+xml"/><item><title>Rebuilding Society on Meaning</title><link>https://mwalton.me/notes/meaning/</link><pubDate>Thu, 16 Mar 2023 00:00:00 +0000</pubDate><guid>https://mwalton.me/notes/meaning/</guid><description>Rebuilding Society on Meaning
Ideas an interesting modeling approach could be combining an LM that converses with users encouraging them to introspect on their values combined w/ a theory of mind that the model constructs (and persists) of them between use sessions (this could also get quite dark) i like the idea of trying to align recommenders w/ what users actually value, however I&amp;rsquo;m concerned about how the existing incentive structure (advertising / attention economy / surveillance capitalism) will incorporate research in this area.</description></item><item><title>Alignment</title><link>https://mwalton.me/thoughts/alignment/</link><pubDate>Mon, 06 Mar 2023 00:00:00 +0000</pubDate><guid>https://mwalton.me/thoughts/alignment/</guid><description>ðŸš§ TODO ðŸš§
Central themes [[thoughts/participatory-design]] [[thoughts/transparency]] [[thoughts/biomimicry]] Contestable AI contestable.ai Meaning Alignment there are some interesting emerging ideas around [[notes/meaning|meaning]] aligned machine learning; I&amp;rsquo;m cautiously optimistic on this direction depending on what organizations get involved coopts the concept</description></item></channel></rss>